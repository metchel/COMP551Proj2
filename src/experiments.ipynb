{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) load in training data from folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pos_examples = [open('../data/train/pos/' + f).read() for f in os.listdir('../data/train/pos')]\n",
    "neg_examples = [open('../data/train/neg/' + f).read() for f in os.listdir('../data/train/neg')]\n",
    "\n",
    "X_ugly = pos_examples + neg_examples\n",
    "y = [1 if i < len(pos_examples) else 0 for i in range(len(pos_examples) + len(neg_examples))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) perform the following steps of preprocessing:\n",
    "    (a) remove punctuation\n",
    "    (b) remove stop words\n",
    "    (c) stem\n",
    "    (d) identify simple negations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"i would see this again but not soon. haven't reccommended to anyone\", \"<br>Hi this?? ;is</br> not. Hello There DUDe the!! coolest thing I've NEVER ever seen\"]\n",
      "['i would see thi again but not soon have not reccommend to anyon', 'hi thi is not hello there dude the coolest thing i have never ever seen']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import math\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "CONTRACTIONS = {\n",
    "    \"aint\": \"is not\",\n",
    "    \"arent\": \"are not\",\n",
    "    \"cant\": \"cannot\",\n",
    "    \"cantve\": \"cannot have\",\n",
    "    \"cause\": \"because\",\n",
    "    \"couldve\": \"could have\",\n",
    "    \"couldnt\": \"could not\",\n",
    "    \"couldntve\": \"could not have\",\n",
    "    \"didnt\": \"did not\",\n",
    "    \"doesnt\": \"does not\",\n",
    "    \"dont\": \"do not\",\n",
    "    \"hadnt\": \"had not\",\n",
    "    \"hadntve\": \"had not have\",\n",
    "    \"hasnt\": \"has not\",\n",
    "    \"havent\": \"have not\",\n",
    "    \"hed\": \"he would\",\n",
    "    \"hedve\": \"he would have\",\n",
    "    \"hell\": \"he will\",\n",
    "    \"hellve\": \"he he will have\",\n",
    "    \"hes\": \"he is\",\n",
    "    \"howd\": \"how did\",\n",
    "    \"howdy\": \"how do you\",\n",
    "    \"howll\": \"how will\",\n",
    "    \"hows\": \"how is\",\n",
    "    \"Id\": \"I would\",\n",
    "    \"Idve\": \"I would have\",\n",
    "    \"Ill\": \"I will\",\n",
    "    \"Ill've\": \"I will have\",\n",
    "    \"Im\": \"I am\",\n",
    "    \"Ive\": \"I have\",\n",
    "    \"id\": \"i would\",\n",
    "    \"idve\": \"i would have\",\n",
    "    \"ill\": \"i will\",\n",
    "    \"illve\": \"i will have\",\n",
    "    \"im\": \"i am\",\n",
    "    \"ive\": \"i have\",\n",
    "    \"isnt\": \"is not\",\n",
    "    \"itd\": \"it would\",\n",
    "    \"itdve\": \"it would have\",\n",
    "    \"itll\": \"it will\",\n",
    "    \"itllve\": \"it will have\",\n",
    "    \"its\": \"it is\",\n",
    "    \"lets\": \"let us\",\n",
    "    \"maam\": \"madam\",\n",
    "    \"maynt\": \"may not\",\n",
    "    \"mightve\": \"might have\",\n",
    "    \"mightnt\": \"might not\",\n",
    "    \"mightntve\": \"might not have\",\n",
    "    \"mustve\": \"must have\",\n",
    "    \"mustnt\": \"must not\",\n",
    "    \"mustntve\": \"must not have\",\n",
    "    \"neednt\": \"need not\",\n",
    "    \"needntve\": \"need not have\",\n",
    "    \"oclock\": \"of the clock\",\n",
    "    \"oughtnt\": \"ought not\",\n",
    "    \"oughtntve\": \"ought not have\",\n",
    "    \"shant\": \"shall not\",\n",
    "    \"shant\": \"shall not\",\n",
    "    \"shantve\": \"shall not have\",\n",
    "    \"shed\": \"she would\",\n",
    "    \"shedve\": \"she would have\",\n",
    "    \"shell\": \"she will\",\n",
    "    \"shellve\": \"she will have\",\n",
    "    \"shes\": \"she is\",\n",
    "    \"shouldve\": \"should have\",\n",
    "    \"shouldnt\": \"should not\",\n",
    "    \"shouldntve\": \"should not have\",\n",
    "    \"sove\": \"so have\",\n",
    "    \"sos\": \"so as\",\n",
    "    \"thatd\": \"that would\",\n",
    "    \"thatdve\": \"that would have\",\n",
    "    \"thats\": \"that is\",\n",
    "    \"thered\": \"there would\",\n",
    "    \"wasnt\": \"was not\",\n",
    "    \"werent\": \"were not\",\n",
    "    \"wont\": \"will not\",\n",
    "    \"wontve\": \"will not have\",\n",
    "    \"wouldnt\": \"would not\",\n",
    "    \"wouldntve\": \"would not have\",\n",
    "}\n",
    "def get_contractions(token):\n",
    "    if token in CONTRACTIONS.keys():\n",
    "        token = CONTRACTIONS[token]\n",
    "    \n",
    "    return token\n",
    "        \n",
    "def clean(X):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    X_clean = []\n",
    "    negative = ['not']\n",
    "    exclams = []\n",
    "    lengths = []\n",
    "    uppercases = []\n",
    "    REPLACE_WITH_EMPTY = re.compile(\"(\\,)|(\\:)|(\\;)|(\\()|(\\))|(\\[)|(\\])|(\\?)|(\\!)|(\\')\")\n",
    "    REPLACE_WITH_SPACE = re.compile(\"(\\-)|(\\.)\")\n",
    "    for x_i in X:\n",
    "        x_i = x_i.lower()\n",
    "        x_i = re.sub(r'<.*?>', ' ', x_i)\n",
    "        x_i = REPLACE_WITH_EMPTY.sub('', x_i)\n",
    "        x_i = REPLACE_WITH_SPACE.sub(' ', x_i)\n",
    "        tokens = word_tokenize(x_i)\n",
    "        tokens = [get_contractions(t) for t in tokens]\n",
    "        restring = ' '.join(tokens)\n",
    "        tokens = word_tokenize(restring)\n",
    "        clean = [stemmer.stem(w) for w in tokens]\n",
    "        #clean = [w if not w == 'film' else 'movi' for w in clean]\n",
    "        #negated = [clean[i-1] + '_' + clean[i] if clean[i-1] in negative else clean[i] for i in range(len(clean))]\n",
    "        #negated = [w for w in negated if w not in negative]\n",
    "        #negated = [w for w in negated if w not in negative]\n",
    "        #pos = nltk.pos_tag(remove_negative)\n",
    "        #tagged = [w[0] for w in pos if w[1] in set(['NN', 'RB', 'JJS', 'VBP', 'VBN'])]\n",
    "        #clean_2 = \" \".join(tagged)\n",
    "        #X_clean.append(' '.join(negated))\n",
    "        X_clean.append(' '.join(clean))\n",
    "    return X_clean\n",
    "\n",
    "def test_clean(test_text_list):\n",
    "    print(test_text_list)\n",
    "    test_text_list = clean(test_text_list)\n",
    "    print(test_text_list)\n",
    "    \n",
    "test_clean(['i would see this again but not soon. haven\\'t reccommended to anyone', '<br>Hi this?? ;is</br> not. Hello There DUDe the!! coolest thing I\\'ve NEVER ever seen'])\n",
    "#print(X_ugly[:1])\n",
    "X = clean(X_ugly)\n",
    "#print(X[:1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) split the dataset into 80% train, 20% validate\n",
    "#### (4) TFIDF vectorization \n",
    "        - played around with parameters, using unigrams/bigrams and ~75k features is best\n",
    "#### (5) Grid search for Logistic regression parameter tuning\n",
    "        - varied C (inverse regulatization) and penalty (l1 or l2 regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "train_X, validate_X, train_y, validate_y = train_test_split(X_ugly, y, test_size=0.2)\n",
    "\n",
    "def mutual_info(X, y, words):\n",
    "        if isinstance(X, list):\n",
    "            X = np.array(X)\n",
    "        n, m = X.shape\n",
    "        print(len(words))\n",
    "        class_counts = {0: 0, 1: 0}\n",
    "        feature_counts = {0: np.zeros(m), 1: np.zeros(m)}\n",
    "        sqr_diff = []\n",
    "        class_probabilities = {}\n",
    "        feature_probabilities = {}\n",
    "\n",
    "        for y_i in y:\n",
    "            class_counts[y_i] += 1\n",
    "\n",
    "        sparse_matrix = csr_matrix(X).nonzero()\n",
    "        (row, col) = sparse_matrix\n",
    "        for i in range(len(row)):\n",
    "            c = y[row[i]]\n",
    "            feature_counts[c][col[i]] += 1\n",
    "\n",
    "        class_probabilities = {0: class_counts[0]/float(n), 1: class_counts[1]/float(n)}\n",
    "        feature_probabilities = {\n",
    "            0: [(feature_count + 1)/float(class_counts[0] + 2) for feature_count in feature_counts[0]],\n",
    "            1: [(feature_count + 1)/float(class_counts[1] + 2) for feature_count in feature_counts[1]]\n",
    "        }\n",
    "        info = []\n",
    "        for i in range(m):\n",
    "            prob_word_0 = feature_probabilities[0][i]\n",
    "            prob_word_1 = feature_probabilities[1][i]\n",
    "            prob_word = prob_word_0 + prob_word_1\n",
    "            \n",
    "            prob_0_1 = prob_word_0*math.log(((prob_word_0)/(prob_word*class_probabilities[0])))\n",
    "            prob_0_0 = (1 - prob_word_0)*math.log((1 - prob_word_0)/(prob_word*class_probabilities[0]))\n",
    "            \n",
    "            prob_1_1 = prob_word_1*math.log((prob_word_1)/(prob_word*class_probabilities[1]))\n",
    "            prob_1_0 = (1 - prob_word_1)*math.log((1 - prob_word_1))/(prob_word*class_probabilities[1])\n",
    "            \n",
    "            info.append((words[i], sum([prob_0_1, prob_0_0, prob_1_1, prob_1_0])))\n",
    "        return info\n",
    "    \n",
    "def count_questions(X):\n",
    "    return [x_i.count('?')/float(len(x_i.split()) + 1) for x_i in X]\n",
    "\n",
    "questions_train, questions_validate = count_questions(train_X), count_questions(validate_X)\n",
    "\n",
    "def get_vocab(X_t, y_t):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    vectorizer = CountVectorizer(binary=True, analyzer='word', ngram_range=(1, 1), strip_accents='ascii')\n",
    "    vectorizer_2 = CountVectorizer(binary=True, analyzer='word', ngram_range=(2, 2), strip_accents='ascii')\n",
    "    \n",
    "    single_counts = vectorizer.fit_transform(X_t)\n",
    "    double_counts = vectorizer_2.fit_transform(X_t)\n",
    "    single_vocab = vectorizer.get_feature_names()\n",
    "    double_vocab = vectorizer_2.get_feature_names()\n",
    "    \n",
    "    info_1 = mutual_info(single_counts, y_t, single_vocab)\n",
    "    info_2 = mutual_info(double_counts, y_t, double_vocab)\n",
    "    \n",
    "    info_1 = sorted(info_1, key = lambda x: x[1])\n",
    "    info_2 = sorted(info_2, key = lambda x: x[1])\n",
    "    \n",
    "    info_all = info_1 + info_2\n",
    "    info_all = sorted(info_all, key = lambda x: x[1])\n",
    "    info_all = [x[0] for x in info_all]\n",
    "    return info_all\n",
    "    \n",
    "#VOCAB = get_vocab(train_X + validate_X, train_y + validate_y)[:500000]\n",
    "#print(VOCAB[:1000])\n",
    "\n",
    "#vect_tfidf = TfidfTransformer()\n",
    "\n",
    "#print(vect_tfidf.get_feature_names())\n",
    "#nb = BernoulliNB()\n",
    "#nb.fit(train_features, train_y)\n",
    "\n",
    "#lr = LogisticRegression(solver='liblinear')\n",
    "\n",
    "#lr_cv = GridSearchCV(lr, grid, cv=10, error_score='raise')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) Include both training and validation data in the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from scipy.sparse import vstack, hstack\n",
    "\n",
    "vect_count = CountVectorizer(binary=False, dtype=int, analyzer='word', ngram_range=(1, 3), max_features=1000000)\n",
    "counts = vect_count.fit_transform(train_X)\n",
    "#print(vect_count.get_feature_names()[400000:4001000])\n",
    "#overlap = set(vect_count.get_feature_names()).difference(set(VOCAB))\n",
    "#print(overlap)\n",
    "vect_tfidf = TfidfTransformer()\n",
    "\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "\n",
    "grid = {'C': [40000, 50000, 100000, 500000]}\n",
    "\n",
    "lr_cv = GridSearchCV(lr, grid, cv=10, error_score='raise')\n",
    "\n",
    "#vocabs = VOCAB[:int(i*len(VOCAB))]\n",
    "#print(len(vocabs))\n",
    "#train_features = hstack(((np.array(questions_train)[:,None], vect_tfidf.fit_transform(counts))))\n",
    "#validate_features = hstack((np.array(questions_validate)[:,None], vect_tfidf.transform(vect_count.transform(validate_X))))\n",
    "train_features = vect_tfidf.fit_transform(counts)\n",
    "validate_counts = vect_count.transform(validate_X)\n",
    "validate_features = vect_tfidf.transform(validate_counts)\n",
    "lr_cv.fit(train_features, train_y)\n",
    "\n",
    "print(lr_cv.cv_results_['mean_test_score'])\n",
    "\n",
    "preds = lr_cv.best_estimator_.predict(validate_features)\n",
    "print('{}'.format(accuracy_score(validate_y, preds)))\n",
    "\n",
    "#all_features = vstack((train_features, validate_features))\n",
    "#all_y = train_y + validate_y\n",
    "\n",
    "all_features = vect_tfidf.fit_transform(vect_count.fit_transform(X_ugly))\n",
    "\n",
    "lr_cv.fit(all_features, y)\n",
    "print(lr_cv.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "test = {}\n",
    "for n in range(25000):\n",
    "    filename = '../data/test/{}.txt'.format(n)\n",
    "    test[n] = open(filename, 'r').read()\n",
    "    \n",
    "test_X = list(test.values())\n",
    "test_X = clean(test_X)\n",
    "print(test_X[0])\n",
    "\n",
    "test_counts = vect_count.transform(test_X)\n",
    "\n",
    "#sum_words = test_counts.sum(axis=0)\n",
    "#words_freq = [(word, sum_words[0, idx]) for word, idx in vect_count.vocabulary_.items()]\n",
    "#words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "#print(words_freq[:100])\n",
    "\n",
    "test_features = vect_tfidf.transform(test_counts)\n",
    "predictions = lr_cv.best_estimator_.predict(test_features)\n",
    "\n",
    "print(test_features.shape)\n",
    "with open('results.csv', 'w') as results:\n",
    "    writer = csv.writer(results, delimiter=',')\n",
    "    writer.writerow(['Id', 'Category'])\n",
    "    for i in range(25000):\n",
    "        writer.writerow([i, predictions[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "all_y = train_y + validate_y\n",
    "documents = [TaggedDocument(doc.split(), [i, all_y[i]]) for i, doc in enumerate(train_X + validate_X)]\n",
    "model = Doc2Vec(documents, vector_size=10, window=2, min_count=1, workers=1)\n",
    "print(model.docvecs[0])\n",
    "\n",
    "feats = [list(model.docvecs[i]) for i in range(len(model.docvecs))]\n",
    "\n",
    "X_training = feats[:len(train_X)]\n",
    "X_validating = feats[len(train_X):]\n",
    "X_training_sparse = hstack((train_features, X_training))\n",
    "X_validating_sparse = hstack((validate_features, X_validating))\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(X_training_sparse)\n",
    "pred = lr.predict(X_validating_sparse)\n",
    "print(accuracy_score(validate_y, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(model.wv.most_similar('movi'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (7) Calculate test results and write to file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
