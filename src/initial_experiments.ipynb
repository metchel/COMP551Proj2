{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pos_examples = [open('../data/train/pos/' + f).read() for f in os.listdir('../data/train/pos')]\n",
    "neg_examples = [open('../data/train/neg/' + f).read() for f in os.listdir('../data/train/neg')]\n",
    "\n",
    "X = pos_examples + neg_examples\n",
    "y = [1 if i < len(pos_examples) else 0 for i in range(len(pos_examples) + len(neg_examples))]\n",
    "\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X, y, train_size=0.8, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1384170\n",
      "[('movi', 40162), ('film', 38285), ('one', 21849), ('like', 17694), ('it', 16081), ('time', 12506), ('good', 12291), ('thi', 12065), ('make', 11756), ('charact', 11262), ('get', 11217), ('watch', 11154), ('see', 11139), ('would', 10750), ('even', 10329), ('stori', 10261), ('realli', 9309), ('well', 8714), ('scene', 8313), ('look', 8007), ('much', 7852), ('show', 7760), ('end', 7641), ('could', 7521), ('bad', 7477), ('peopl', 7414), ('go', 7366), ('great', 7266), ('also', 7239), ('first', 7237), ('love', 7184), ('think', 7087), ('way', 7041), ('play', 6997), ('act', 6983), ('made', 6579), ('thing', 6506), ('know', 5956), ('say', 5938), ('seem', 5754), ('work', 5679), ('come', 5543), ('th', 5529), ('plot', 5515), ('two', 5512), ('in', 5492), ('actor', 5410), ('year', 5380), ('seen', 5313), ('mani', 5289), ('want', 5275), ('take', 5196), ('never', 5187), ('littl', 5127), ('best', 5117), ('life', 5075), ('tri', 4999), ('man', 4843), ('ever', 4784), ('better', 4606), ('give', 4570), ('still', 4487), ('perform', 4384), ('if', 4350), ('find', 4303), ('feel', 4250), ('he', 4199), ('part', 4185), ('director', 4130), ('back', 4128), ('ve', 4093), ('use', 4017), ('someth', 4014), ('actual', 4006), ('interest', 3899), ('lot', 3834), ('real', 3756), ('old', 3657), ('though', 3647), ('cast', 3646), ('woman', 3623), ('re', 3590), ('new', 3532), ('star', 3493), ('live', 3487), ('10', 3451), ('guy', 3449), ('role', 3432), ('noth', 3402), ('enjoy', 3391), ('anoth', 3388), ('music', 3365), ('point', 3359), ('funni', 3332), ('start', 3278), ('set', 3240), ('origin', 3210), ('girl', 3201), ('world', 3174), ('everi', 3165)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "vectorizer = CountVectorizer(binary=False, analyzer='word', ngram_range=(1, 2)).fit(X_train)\n",
    "\n",
    "training_features = vectorizer.transform(X_train)\n",
    "validating_features = vectorizer.transform(X_validate)\n",
    "\n",
    "sum_words = training_features.sum(axis=0)\n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "print(len(vectorizer.get_feature_names()))\n",
    "print(words_freq[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from naive_bayes import BernoulliNaiveBayes\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXPERIMENT 0: Bernoulli vs. Multinomial Naive Bayes with and without stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING 5.0% of most frequent word features.\n",
      "34604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.899\tMultinomial: 0.8828\n",
      "USING 10.0% of most frequent word features.\n",
      "69208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.9004\tMultinomial: 0.8846\n",
      "USING 15.0% of most frequent word features.\n",
      "103812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.8998\tMultinomial: 0.8864\n",
      "USING 20.0% of most frequent word features.\n",
      "138417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.9018\tMultinomial: 0.8846\n",
      "USING 25.0% of most frequent word features.\n",
      "173021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.902\tMultinomial: 0.8852\n",
      "USING 30.0% of most frequent word features.\n",
      "207625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.902\tMultinomial: 0.8846\n",
      "USING 35.0% of most frequent word features.\n",
      "242229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.903\tMultinomial: 0.8852\n",
      "USING 40.0% of most frequent word features.\n",
      "276834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.9018\tMultinomial: 0.8854\n",
      "USING 45.0% of most frequent word features.\n",
      "311438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.9016\tMultinomial: 0.8856\n",
      "USING 50.0% of most frequent word features.\n",
      "346042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.9014\tMultinomial: 0.8858\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "def clean(X, params={'lemmatize': True, 'stem': True}):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    X_clean = []\n",
    "    negative = set(['not', 'isnt', 'no', 'n\\'t', 'never', 'can\\'t', 'won\\'t', 'don\\'t', 'havn\\'t', 'didn\\'t', 'hasn\\'t', 'wouldn\\'t', 'couldn\\'t', 'shouldn\\'t'])\n",
    "    for x_i in X:\n",
    "        x_i = re.sub(r'<.*?>', '', x_i)\n",
    "        tokens = word_tokenize(x_i)\n",
    "        clean = [w for w in tokens if w not in stop_words]\n",
    "        if params['stem'] and not params['lemmatize']:\n",
    "            clean = [stemmer.stem(w) for w in clean]\n",
    "        if params['lemmatize'] and not params['stem']:\n",
    "            clean = [lemmatizer.lemmatize(w) for w in clean]\n",
    "        if params['stem'] and params['lemmatize']:\n",
    "            clean = [lemmatizer.lemmatize(stemmer.stem(w)) for w in clean]\n",
    "        negated = ['not_' + clean[i] if clean[i-1] in negative else clean[i] for i in range(len(clean))]\n",
    "        remove_negative = [w for w in negated if w not in negative]\n",
    "        clean_2 = \" \".join(remove_negative)\n",
    "        X_clean.append(' '.join(clean))\n",
    "    return X_clean\n",
    "\n",
    "#X_train, X_validate = clean(X_train), clean(X_validate)\n",
    "\n",
    "num_words = vectorizer.vocabulary_.__len__()/2\n",
    "\n",
    "for x in [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]:\n",
    "    print('USING {}% of most frequent word features.'.format(x*100))\n",
    "    features = int(num_words*x)\n",
    "    print(features)\n",
    "    vectorizer = CountVectorizer(binary=False, analyzer='word', max_features = features, stop_words=None, ngram_range=(1, 2))\n",
    "    training_features = vectorizer.fit_transform(X_train)\n",
    "    tfidf = TfidfTransformer()\n",
    "    normalizer = Normalizer()\n",
    "    train_tfidf = normalizer.fit_transform(tfidf.fit_transform(training_features))\n",
    "    validating_features = vectorizer.transform(X_validate)\n",
    "    validate_tfidf = normalizer.transform(tfidf.transform(validating_features))\n",
    "    model_lr = LogisticRegression(C=10)\n",
    "    model_lr.fit(train_tfidf, y_train)\n",
    "    predictions = model_lr.predict(validate_tfidf)\n",
    "    accuracy = accuracy_score(y_validate, predictions)\n",
    "    model_multi_nb = MultinomialNB()\n",
    "    model_multi_nb.fit(train_tfidf, y_train)\n",
    "    predictions_2 = model_multi_nb.predict(validate_tfidf)\n",
    "    accuracy_2 = accuracy_score(y_validate, predictions_2)\n",
    "    \n",
    "    results['bernoulli'].append(accuracy_2)\n",
    "    results['multinomial'].append(accuracy_2)\n",
    "    \n",
    "    print('LR: {}\\tMultinomial: {}'.format(accuracy, accuracy_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXPERIMENT 1: Stemming vs. lemmatizing and removal of html tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM FEATURES: 69267\n",
      "BERNOULLI PARAMS: {'lemmatize': False, 'stem': False} ACCURACY: 0.854\n",
      "MULTINOMIAL PARAMS: {'lemmatize': False, 'stem': False} ACCURACY: 0.8612\n",
      "NUM FEATURES: 65460\n",
      "BERNOULLI PARAMS: {'lemmatize': True, 'stem': False} ACCURACY: 0.8548\n",
      "MULTINOMIAL PARAMS: {'lemmatize': True, 'stem': False} ACCURACY: 0.8616\n",
      "NUM FEATURES: 53407\n",
      "BERNOULLI PARAMS: {'lemmatize': False, 'stem': True} ACCURACY: 0.8514\n",
      "MULTINOMIAL PARAMS: {'lemmatize': False, 'stem': True} ACCURACY: 0.8628\n",
      "NUM FEATURES: 53296\n",
      "BERNOULLI PARAMS: {'lemmatize': True, 'stem': True} ACCURACY: 0.8522\n",
      "MULTINOMIAL PARAMS: {'lemmatize': True, 'stem': True} ACCURACY: 0.8622\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "for params in [(False, False), (True, False), (False, True), (True, True)]:\n",
    "    clean_params = {\n",
    "        'lemmatize': params[0],\n",
    "        'stem': params[1]\n",
    "    }\n",
    "    X_train_clean, X_validate_clean = clean(X_train, clean_params), clean(X_validate, clean_params)\n",
    "    vectorizer = CountVectorizer(binary=False, analyzer='word')\n",
    "    t_features = vectorizer.fit_transform(X_train_clean)\n",
    "    v_features = vectorizer.transform(X_validate_clean)\n",
    "    \n",
    "    print('NUM FEATURES: {}'.format(vectorizer.vocabulary_.__len__()))\n",
    "    \n",
    "    model_bernoulli_nb = BernoulliNB()\n",
    "    model_bernoulli_nb.fit(t_features, y_train)\n",
    "    predictions = model_bernoulli_nb.predict(v_features)\n",
    "    accuracy = accuracy_score(y_validate, predictions)\n",
    "    \n",
    "    model_multi_nb = MultinomialNB()\n",
    "    model_multi_nb.fit(t_features, y_train)\n",
    "    predictions_2 = model_multi_nb.predict(v_features)\n",
    "    accuracy_2 = accuracy_score(y_validate, predictions_2)\n",
    "    \n",
    "    print('BERNOULLI PARAMS: {} ACCURACY: {}'.format(clean_params, accuracy))\n",
    "    print('MULTINOMIAL PARAMS: {} ACCURACY: {}'.format(clean_params, accuracy_2))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXPERIMENT 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
